# -*- coding: utf-8 -*-
"""TIAT1_RL_HuberLoss.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/12jZVUwFKVI6Ap0NWQc73ymXV6V3GiLze

# Huber Loss
ofrece lo mejor de ambos mundos al equilibrar el MSE y el MAE juntos.
"""

import numpy as np
import matplotlib.pyplot as plt

x_ds = [i for i in range(50)]
y_ds = [ i + np.random.normal(0,3) for i in x_ds ]
plt.plot(x_ds,y_ds, '*')

"""$h(x_i) = w_0 + w_1x_i$"""

def h(x,w):
  return w[0] + w[1]*x

w = np.random.rand(2)
print(w)
plt.plot(x_ds,y_ds, '*')
plt.plot(x_ds,[h(i,w) for i in x_ds])

"""Huber Loss

$Error =  {\begin{matrix} \frac{1}{2n}\sum_{i=0}(y_i - h(x_i))^2  & & for(y_i - h(x_i)) \le \delta \end{matrix}}$

$Error =  {\begin{matrix} \sum_{i=0}\delta|y_i - h(x_i)|-\frac{1}{2}*\delta^2 & & otherside \end{matrix}}$
"""

def ErrorHL(y,x,w,delta=1.0):
  return sum([np.where(abs(e[0] - h(e[1],w)) <= delta, 0.5*(e[0] - h(e[1],w))**2, delta * (abs(e[0] - h(e[1],w)) - 0.5 * delta)) for  e in zip(y,x) ])/(len(y))

delta=1.5
print("Error Huber Loss:" + str(ErrorHL(y_ds, x_ds,w,delta)))

"""Calcular las derivadas 

$Error =  {\begin{matrix}\frac{1}{2n}\sum_{i=0}(y_i - h(x_i))Â²  & & for(y_i - h(x_i)) \le \delta \end{matrix}}$

$Error =  {\begin{matrix}\frac{1}{n}\sum_{i=0}|y_i - h(x_i)| & & otherside \end{matrix}}$

$\frac{ \partial Error}{\partial w_0}  = \frac{1}{n}\sum_{i=0}*(y_i - h(x_i))*(-1) $

$\frac{ \partial Error}{\partial w_0}  = \frac{1}{n}\sum_{i=0}\frac{y_i - h(x_i)}{|y_i - h(x_i)|} * -1 $

$\frac{ \partial Error}{\partial w_1}  = \frac{1}{n}\sum_{i=0}*(y_i - h(x_i))*(-x) $

$\frac{ \partial Error}{\partial w_1}  = \frac{1}{n}\sum_{i=0}\frac{y_i - h(x_i)}{|y_i - h(x_i)|}*(-x) $
"""

def gradHL(y,x,w,delta=1.0):
  grad_w0 = sum([np.where(abs(e[0] - h(e[1],w)) <= delta, (e[0] - h(e[1],w))*(-1), (e[0] - h(e[1],w))/abs(e[0] - h(e[1],w)) *(-1)) for  e in zip(y,x) ])/(len(y))
  grad_w1 = sum([np.where(abs(e[0] - h(e[1],w)) <= delta, (e[0] - h(e[1],w))*(-e[1]), (e[0] - h(e[1],w))/abs(e[0] - h(e[1],w)) *(-e[1])) for  e in zip(y,x) ])/(len(y))
  return grad_w0, grad_w1

def train(x_ds, y_ds, w, epochs, alpha, delta):
  list_error = []
  time = []
  for i in range(epochs):
    Err = ErrorHL(y_ds,x_ds,w, delta)
    list_error.append(Err)
    time.append(i)
    grad_w0, grad_w1 = gradHL(y_ds,x_ds,w, delta)
    w[0] = w[0] - alpha*grad_w0
    w[1] = w[1] - alpha*grad_w1
  return time, list_error

print(w)
t, le = train(x_ds,y_ds, w, 100000, 0.0007, delta)
print(w)
plt.plot(t,le)

plt.plot(x_ds,y_ds, '*')
plt.plot(x_ds,[h(i,w) for i in x_ds])